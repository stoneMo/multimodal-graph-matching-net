<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    /* font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; */
    font-family: "Times New Roman", Serif;
    font-weight:300;
    font-size:20px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  h2 {
    font-weight:200;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Start : Google Analytics Code -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-64069893-4');
</script>
<!-- End : Google Analytics Code -->

<script type="text/javascript" src="resources/hidebib.js"></script>
<!-- <script type="text/javascript" src="resources/LaTeXMathML.js"></script> -->
<!-- End : MathLatex Package -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="resources/seal_icon.png">
  <title>Referring 3D Instances via multimodal graph matching</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://pathak22.github.io/modular-assemblies/" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="Self-Assembling Morphologies" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity" />
  <meta property="og:description" content="Pathak, Lu, Darrell, Isola, Efros. Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity. 2019." />
  <meta property="og:url" content="https://pathak22.github.io/modular-assemblies/" />
  <meta property="og:image" content="https://pathak22.github.io/modular-assemblies/resources/teaser.jpg" />
  <meta property="og:video" content="https://www.youtube.com/v/ngCIB-IWD8E" />

  <meta property="article:publisher" content="http://people.eecs.berkeley.edu/~pathak/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity" />
  <meta name="twitter:description" content="Pathak, Lu, Darrell, Isola, Efros. Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity. 2019." />
  <meta name="twitter:url" content="https://pathak22.github.io/modular-assemblies/" />
  <meta name="twitter:image" content="https://pathak22.github.io/modular-assemblies/resources/teaser.jpg" />
  <meta name="twitter:label1" content="Written by" />
  <meta name="twitter:data1" content="Deepak Pathak" />
  <meta name="twitter:label2" content="Filed under" />
  <meta name="twitter:data2" content="" />
  <meta name="twitter:site" content="@pathak" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://pathak22.github.io/modular-assemblies/resources/teaser.jpg" />
  <meta name="twitter:player" content="https://www.youtube.com/embed/ngCIB-IWD8E?rel=0&showinfo=0" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">Referring 3D Instances via Multimodal Graph Matching</center><br/>
      <table align=center width=800px>
      <tr>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://www.ri.cmu.edu/ri-people/jianchun-chen/" target="_blank">Jianchun Chen *</a></span></center></td>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://stonemo.github.io/" target="_blank">Shentong Mo *</a></span></center></td>
        <!-- <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a></span></center></td>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://www.eecs.mit.edu/people/faculty/phillip-isola/" target="_blank">Phillip Isola</a></span></center></td>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://people.eecs.berkeley.edu/~efros/" target="_blank">Alexei A. Efros</a></span></center></td> -->
      <tr/>
      <tr>
        <td align=center width=200px>
        <center><span style="font-size:20px">Carnegie Mellon University</span></center></td>
        <td align=center width=200px>
        <center><span style="font-size:20px">Carnegie Mellon University</span></center></td>
        <!-- <td align=center width=200px>
        <center><span style="font-size:20px">UC Berkeley</span></center></td>
        <td align=center width=200px>
        <center><span style="font-size:20px">MIT</span></center></td>
        <td align=center width=200px>
        <center><span style="font-size:20px">UC Berkeley</span></center></td> -->
      <tr/>
      </table>
      <table align=center width=600px style="padding-top:0px;padding-bottom:20px">
          <tr>
            <td align=center width=600px><center><span style="font-size:20px">* equal contribution</span></center></td>
          <tr/>
      </table>
      <table align=center width=700px>
          <tr>
            <td align=center width=700px><center><span style="font-size:22px">Course Project at <a href="https://visual-learning.cs.cmu.edu/">16824: Visual Learning and Recognition, 2021 Spring</a></span></center></td>
          <tr/>
          <!-- <tr>
            <td align=center width=700px><center><span style="font-size:22px">Winner of the <a href="https://virtualcreatures.github.io/">Virtual Creatures Competition at GECCO 2019</a></span></center></td>
          <tr/> -->
      </table><br/>
      <table align=center width=700px>
          <tr>
            <!-- <td align=center width=100px><center><span style="font-size:28px"><a href="resources/assemblies.pdf">[Report]</a></span></center></td> -->
            <td align=center width=100px><center><span style="font-size:28px"><a href="resources/slides.pdf">[Slides]</a></span></center></td>
            <!-- <td align=center width=100px><center><span style="font-size:28px"><a href="resources/poster.pdf">[Poster]</a></span></center></td> -->
            <td align=center width=100px><center><span style="font-size:28px"><a href='https://github.com/pathak22/modular-assemblies/'>[GitHub Code]</a></span></center></td>
          <tr/>
      </table><br/>

<!--       <center><h2>Project Video</h2></center> -->
      <table align=center width=300px>
      <tr><td align=center width=300px>
      <iframe width="768" height="432" src="https://www.youtube.com/embed/ngCIB-IWD8E?rel=0" frameborder="0" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br>

      <center><h1>Introduction</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
      <!-- <div class="col col-wrapper"> -->
        3D referring (grounding) is a newly defined vision task aiming at indicating target object describedby nature languages from a 3D scene. 
        While some research have achieved great success in referring 2D instances in a image or video context, predicting desired 3D instance is still an open problem, since 3D scene provides more complicated visual information and spatial relation among differentobjects. 
        In addition, there are countless sentences that can describe the position of a 3D object, which makes the language harder to understand. 
        However, this task attracts researcher’s attention with rich potential to apply in human-computer interaction, navigation, etc.  

        <!-- <blockquote>$\displaystyle{\hat{N}^l_i = \phi_n(N^l_i); \hat{E}^l_t = \phi_e(E^l_i)}$</blockquote> -->
        <!-- $\\$\alpha + \\$\beta = \\$(\alpha + \beta).$ -->
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">
      <!-- <div class="col col-wrapper"> -->
        To approach this task, previous efforts mainly contain two steps: 
        1) they first segment 3D instancesand learn instance-level visual representation throughout the scene; 
        2) then integrate the visual featurewith input language features and determine the instance(s) that best align the feature of input language. 
        This pipeline successfully addresses many simple query sentence, but easily failed when the querysentences describe complex spatial relation between objects 
        (<i>e.g.</i>, <em>'Standing in the middle of the room,looking at the two windows, the one to the right'</em>). 
        One critical issue that accounts is, the structureand relation of language tokens are equally important compared with the vision part, but harderto understand. 
        By treating language as a linear structure, neither max-pooling [1,2] nor attentionmechanism [3] is sufficient to learn spatial relationship in language effectively.      
        <!-- \(\alpha\) -->
        <!-- \(\hat{N}^l_i = \phi_n(N^l_i); \hat{E}^l_t = \phi_e(E^l_i)\) -->
        <!-- \[\hat{N}^l_i = \phi_n(N^l_i); \hat{E}^l_t = \phi_e(E^l_i)\] -->

        <!-- <blockquote>$\displaystyle{\hat{N}^l_i = \phi_n(N^l_i); \hat{E}^l_t = \phi_e(E^l_i)}$</blockquote> -->

        <!-- $\\$\alpha + \\$\beta = \\$(\alpha + \beta).$ -->
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">
        This triggers our motivation in proposing this Multimodal Graph Matching Network. 
        Our main intuition is to introduce prior knowledge from semantic parser to understand language structure. 
        Particularly, we first generate a parse-tree and re-arrange the root and leaf node given a easy attainable language classifier. 
        With such a well-organized language parsing tree and a canonical vision graph from Graph Neural Network, we matches the parsing tree and the k-NN sub-tree of candidate instance with our proposed similarity metric. 
        A contrastive loss supervises the ground truth instance to have the largest similarity with the parsing tree of query sentence. 
        Unlike \cite{chen2019scanrefer,achlioptas2020referit3d,huang2021text}, our graph matching procedure imposes the strong constraint of the existence of every language component and relation in the visual graph. 
        The noun of locality gains the largest attention as it is explicitly separated out. 
        The matching result is also easier to be comprehended.
      
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">

        The main efforts of our report are three folds:
        <li>We re-organize the nature language sentences into parse-trees with a dynamic graph convolution network to learn relations among tokens.</li>
        <li>We propose a novel sub-tree distance measurement and successfully integrate graph matching into an end-to-end learnable pipeline.</li>
        <li>We demonstrate the superiority of metric learning loss over classification loss for 3D referring task.</li>
        We further make experimental comparisons with state-of-the-art techniques and analyze the performance.
      </div><br/>
      <br><hr>

      <center><h1>Related Works</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        <b>2D/3D Visual Grounding</b> In recent years, connecting vision to natural language gains its popularity. 
        Visual grounding [4,5] is one of the long standing tasks that aims to localize the instance that is referred by the query sentence. 
        One vital clue is to understand the relationships between objects.Following this insight, researchers deploy powerful Graph Neural Network on visual [6] and language [7] graph processed by preliminary pipelines and extracts discriminative features for alignment.
        The newly released 3D grounding data [1,2] is even closer to our real world scenario. 
        [8] applied agenerative model for conditional generation of 3D models from text in augmented reality applications.
        [1] proposed a concurrent work to localize/discover referred objects in 3D scenes by collectingnatural language. 
        In this work, we closely follow the same setting as [1,2], and focus on identifyinga referred object among instances of the same fine-grained category given the segmented objectinstances in a room.
      </div><br/>
  
      <div style="width:800px; margin:0 auto; text-align=center">

        <b>3D Deep Learning</b>With the success of deep learning, the vision community try to study 3D worldby multi-view [9], voxel [10] and point cloud [11] representation.  
        Among these early practices,PointNet is proved to be efficient and convenient for large-scale 3D scene understanding. 
        On top ofthat, some researchers [12,13] present a detection-free instance segmentation pipeline by shiftingpoints to its instance centroid. 
        In another thread, by advancing recent Sparse Convolution [14] and CNN paradigm such as UNet and ResNet, 3DCNN produces expressive local features in 3D. 
        Our visual learning works closely follow these cutting edges.
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">

        <b>Graph Matching</b> Graph matching [15,16,17] is a widely used techniques that measures thestructural similarity of two instances, especially for image registration and matching. 
        However, how to extend this techniques into an end-to-end trainable pipeline is still an open task. 
        Zanfil et al. [16] first cope with this methods by a bi-stochastic optimization over a 2-order similarity matrix. 
        However,our graph data in the language side is more simple than the keypoint graph generated from ImageNet. 
        Therefore, a greedy algorithm or heuristic algorithm is sufficient for us.
      </div><br/>
      <br><hr>

      <center><h1>Methodology</h1></center>
      <center><h2>Problem Setup</h2></center>

      <div style="width:800px; margin:0 auto; text-align=center">
        Our Multimodal Graph Matching Network takes a 3D point cloud \(P\in \mathbb{R}^{T\times3}=\{p_1,...,p_n\}\) with RGB/normal features \(F\in \mathbb{R}^{T\times4}=\{f_1,...,f_n\}\) and a query sentence as an input. 
        Section \ref{mtg} first use GRU to encode token-wise GloVE feature as \(Q=\{q_1,...,q_k\}\) and then generate a parsing tree \(\mathcal{T}^l=\{N^l_c,(N^l_1,E^l_1),...,(N^l_t,E^l_t)\}\), where \(N,E\) are \(d\) dimension node/edge feature of the parsing tree respectively. 
        \(N_c\) denotes the root of the tree and the rest are leaves. 
        Section \ref{3dis} processes \(P\) and \(F\) into \(r\) instances with corresponding features. 
        We construct a visual graph as \(\mathcal{G}^v=\{N^v_1,...,E^v_{12},...\}\) using k-nearest-neighbor (k-NN). 
        \(\mathcal{G}^v\) is passed through a GNN to become \(\hat{\mathcal{G}}^v\) with spatial relation features extracted. 
        Since structure of parsing tree \(\mathcal{T}^l\) is simpler, we directly use node-wise convolution without image passing and produce \(\hat{\mathcal{T}}^l\). 
        For the \(j\)-th node in \(\hat{\mathcal{G}}^l\), we compute its 1-NN sub-tree as \(\hat{\mathcal{T}}^v_j\). 
        Section \ref{mgm} serves as a similarity metric for \(\hat{\mathcal{T}}^v_j\) and \(\hat{\mathcal{T}}^l\). 
        It reflects the confident value of the \(j\)-th instance being referred by the sentence.
      </div><br/>

      <center><h2>Parsing Tree Generation</h2></center>

      <div style="width:800px; margin:0 auto; text-align=center">
        In order to get rid of linear structures of token-wised embeddings used in previous work, we apply an English dependency parser~\cite{} to the token-wise GloVE feature \(Q=\{q_1,...,q_k\}\) of the description for each sample instance.  
        In this way, the dependency structure of sentences, <i>i.e.</i>, which words depend on (modify or are arguments of) which other words will be learned.
        In Figure \ref{}, we show an example of the dependency tree for the description 'there is a   white toilet. placed in the corner of the bath.'.
        Given the sentence, we generate a parsing tree \(\mathcal{T}^l=\{N^l_c,(N^l_1,E^l_1),...,(N^l_t,E^l_t)\}\), where \(N,E\) are \(d\) dimension node/edge feature of the parsing tree respectively. 
        In this case, we set the center node `toilet' as $N^l_c$, the leaf node `bath' as \(N^l_t\), and the edge `corner' as \(E^l_t\).
      </div><br/>

      <!-- <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=200px>
          <center><a href="resources/parsing_graph.png"><img src = "resources/parsing_graph.png" height="250px"></img></a><br></center>
        </td></tr>
      </table>
      <br/><hr> -->
      <div style="width:800px; margin:0 auto; text-align=center">

      <figure>
        <img src="resources/parsing_graph.png" alt="Trulli" style="width:800px; margin:0 auto; text-align=center">
        <figcaption>Fig.1 A dependency parsing tree for the description <i>`there is a white toilet. placed in the corner of the bath.'</i>.</figcaption>
      </figure>
      
    </div><br/>

      <center><h2>3D Instance Segmentation</h2></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        In this section, we directly adapt state-of-the-art PointGroup \cite{jiang2020pointgroup} as an off-the-shelf backbone. 
        Specifically, given point cloud \(P\) and its point feature \(F\), PointGroup predicts point-wise semantic label \(S\in \mathbb{R}^{n\times1}=\{s_1,...,s_n\}\) and offset \(\Delta\in\mathbb{R}^{n\times3}=\{\Delta p_1,...,\Delta p_n\}\). 
        A clustering algorithm is then carried out to group points with same segmentation label prediction \(s\) and neighboring location \(p+\Delta p\) after deformation into instances \(C=\{C_1,...,C_m\}\). 
        We recommend readers to go over Alg.1 in \cite{jiang2020pointgroup} for details. 
        Note that the instance segmentor is trained separately in our pipeline.
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">
        With the instances segmented, we are able to build a graph \(\mathcal{G}^v\) to fully represent the indoor scene. 
        The nodes of the graph are the instances \(C\), and the edge of the graph is constructed via k-NN.
      </div><br/>


      <center><h2>Vision & Language Feature Encoding</h2></center>

      <div style="width:800px; margin:0 auto; text-align=center">
        Due to the complex structure of the vision graph, we are motivated to use a Dynamic Graph Convolution Network (DGCN) \cite{wang2019dynamic} to extract discriminative instance features considering its relation with neighboring instances. 
        Specifically, the instance (node) feature \(N^t\) is extracted by a sparse convolution network (SCN) and the relation (edge) feature is extracted through a multi-layer perceptron (MLP). 
        Then we use a convolution layer \(\phi\) to couple the edge feature with associated neighboring node feature. 
        DGCN learns a node feature by simultaneously aggregate edge and neighboring node feature with a max-pooling, as shown in equation below.
        
        \[\hat{N}^v_i = \max_j \phi(SCN(N^v_j),MLP([C_i,C_j,C_i-C_j,||C_i-C_j||_2]))\]
      
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">
        As for the language part, since the parsing trees share a simple structure (<i>i.e.</i> a tree with depth equals 
        1), a direct convolutional layers \(\psi\) is carried out learn the node/edge feature without any message passing. 
        The language encoding network is formulated as
        \[\hat{N}^l_i = \phi_n(N^l_i); \hat{E}^l_t = \phi_e(E^l_i)\]
      </div><br/>


      <center><h2>Multimodal Graph Matching</h2></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        In this subsection, we aim at finding the structural similarity of each instances in the vision graph with the generated parsing tree. 
        A feasible idea is to leverage the GCN again to aggregate neighbor features and compare the central features. 
        However, this black-box network loses the clear edge and node correspondence over two graphs/trees and is also less interpretable. 
        Therefore, we first cut the graph into $m$ subtrees for \(m\) instances using k-NN and then define a similarity \(\mathcal{D}\) that measures the similarity of \(\hat{\mathcal{T}}^l\) and \(\hat{\mathcal{T}}^v\). 
        The similarity of two trees is defined as the sum over cosine similarities of corresponding edges and nodes over trees, as shown in Figure \ref{}.

        \[\mathcal{D}(\hat{\mathcal{T}}^v,\hat{\mathcal{T}}^l) = \max_{\mathcal{A}:i\to j} \sum_i \frac{E^l_i\cdot E^v_{j}}{||E^l_i||_2||E^v_j||_2} + \frac{N^l_i\cdot N^v_{j}}{||N^l_i||_2||N^v_j||_2}\]
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">
        Before estimating the similarity, we need to determine the correspondences (<i>i.e.</i> the assignment matrix \(\mathcal{A}\)) first. 
        The criterion is that true correspondences result in largest overall similarity. 
        As we create the visual sub tree by k-NN, there might be redundant neighboring node that does not described in the sentence. 
        However, every component in the language parsing tree is expected to find clear corresponding node and edge. 
        This is essentially a Linear Assignment Problem. Among different variants, we integrate \cite{bertsekas1992auction} into our pipeline and make our network end-to-end trainable.
      </div><br/>

      <center><h2>Loss Function</h2></center>
      
      <div style="width:800px; margin:0 auto; text-align=center">
        Since we consider 3D referring rather a matching and retrieval task then a classification task, we argue that ranking loss is much more proper than standard Cross Entropy loss for this task. 
        It enlarges the gap of similarity scores between positive pairs and negative pairs and makes feature more discriminative. 
        In referencing, we predict 3D instances with similarity score above a threshold, which is naturally effective for query sentences with multi corresponding instances. 
        Empirically, we found the Contrastive loss obtains superior performance against CrossEntropy loss and InfoNCE loss. 
        Given the visual sub-tree of a positive instance and a negative instance, the loss function is formulated as below.

        \[\mathcal{L}(\hat{\mathcal{T}}^v_{+},\hat{\mathcal{T}}^v_{-},\hat{\mathcal{T}}^l) = \max(\mathcal{D}(\hat{\mathcal{T}}^v_{-},\hat{\mathcal{T}}^l)-\mathcal{D}(\hat{\mathcal{T}}^v_{+},\hat{\mathcal{T}}^l)+\alpha,0)\]
        where \(\alpha\) is the hyper-parameter greater than 0.

      </div><br/>
      <br><hr>

      <center><h1>Experiments</h1></center>
      <center><h2>Datasets, Baseline, Metrics</h2></center>

      <div style="width:800px; margin:0 auto; text-align=center">
        We evaluate our method using the ScanRefer dataset. ScanRefer is a dataset is based on ScanNetv2, consisting 51,583 descriptions of 11,046 objects in 1,513 3D indoor scenes. 
        ScanRefer provides training, validation and testing splits. 
        Since the testing labels are not released, the model performance are currently evaluated on the validation set.
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">
        We basically compare our performance with newest state-of-the-arts InstanceRefer \cite{yuan2021instancerefer} and TGNN \cite{huang2021text}. 
        We also provide self-comparison here.
      </div><br/>

      <div style="width:800px; margin:0 auto; text-align=center">
        Following the ScanRefer \cite{chen2019scanrefer}, we use \(Acc@0.25IoU\) and \(Acc@0.5IoU\), 
        where instances are counted positive if and only if the IoU of predicted box and the ground truth box larger than the threshold.
      </div><br/>

      <center><h2>Experiment Settings</h2></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        Our implementation is built on PyTorch.
        We train the network for 32 epochs by using the Adam optimizer with the batch size of 32.
        The initial learning rate is 0.001 with the decay as 0.9 at 10, 20 epoch.
        All experiments are conducted on one NVIDIA 2080Ti GPU.
      </div><br/>

      <center><h2>Comparison with State-of-the-arts</h2></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        Table \ref{tab: exp_sota} reports the comparsion results with state-of-the-arts.

      </div><br/>

      
      <center><h2>Visualization Results</h2></center>

      <div style="width:800px; margin:0 auto; text-align=center">
        Figure \ref{fig: vis_scanrefer} shows visualization results using our proposed model.
      </div><br/>
      <br><hr>

      <center><h1>Discussions</h1></center>

      <div style="width:800px; margin:0 auto; text-align=center">
        TODO
      </div><br/>
      <br><hr>

      <center><h1>Future Work</h1></center>

      <div style="width:800px; margin:0 auto; text-align=center">
      <li>We would like to focus on the actual performance bottleneck of our methods and improve both the backbone module (maybe replace GloVE with pretraining BERT) and processing logic.</li>
      <li>The efficiency bottleneck is another point. The algorithm would be significantly faster if we directly implement a CUDA operator for LAP.</li>
      <li>The effectiveness of our proposed method need to be further examined on the test set of ScanRefer and also Referit3D.</li>
      </div><br/>

      <center><h1>References</h1></center>

      
      <div style="width:800px; margin:0 auto; text-align=center">
      
      <ul style="list-style: none;">
        <li>[1] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner. Scanrefer: 3d object localization inrgb-d scans using natural language. <i>arXiv preprint arXiv:1912.08830</i>, 2019.</li>
        <li>[2] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas.Referit3d:  Neural listeners for fine-grained 3d object identification in real-world scenes.  In <i>European Conference on Computer Vision</i>, pages 422–440. Springer, 2020.
        <li>[3] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graphneural networks for referring 3d instance segmentation. In <i>Proceedings of the AAAI Conferenceon Artificial Intelligence</i>, 2021.</li>
        <li>[4] Daqing Liu, Hanwang Zhang, Feng Wu, and Zheng-Jun Zha.  Learning to assemble neuralmodule tree networks for visual grounding.  In <i>Proceedings of the IEEE/CVF InternationalConference on Computer Vision</i>, pages 4673–4682, 2019.</li>
        <li>[5] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. Modelingrelationships in referential expressions with compositional modular networks. In <i>Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition</i>, pages 1115–1124, 2017</li>
        <li>[6] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein,and Li Fei-Fei. Image retrieval using scene graphs. In <i>Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition</i>, pages 3668–3678, 2015.</li>
        <li>[7] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han.Linguistic structure guided context modeling for referring image segmentation. In <i>European Conference on Computer Vision</i>, pages 59–75. Springer, 2020.</li>
        <li>[8] Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X. Chang, Thomas A. Funkhouser,and Silvio Savarese. Text2shape: Generating shapes from natural language by learning joint embeddings. <i>CoRR, abs/1803.08495</i>, 2018</li>
        <li>[9] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han.Linguistic structure guided context modeling for referring image segmentation. In <i>European Conference on Computer Vision</i>, pages 59–75. Springer, 2020.</li>
        <li>[10] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han.Linguistic structure guided context modeling for referring image segmentation. In <i>European Conference on Computer Vision</i>, pages 59–75. Springer, 2020.</li>
      
      
      
      </ul>
      </div><br/>




      <!-- <center><h1>Introduction</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        This work investigates the joint learning of control and morphology in self-assembling agents. We illustrate our dynamic agents in two environments / tasks: standing up and locomotion. For each of these, we generate several new environment for evaluating zero-shot generalization without any finetuning.
      </div><br/> -->
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><a href="resources/teaser.jpg"><img src = "resources/teaser.jpg" height="450px"></img></a><br></center>
        </td></tr>
      </table>
      <br/><hr>

      <center id="sourceCode"><h1>Source Code and Environment</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
      We have released the PyTorch based implementation and environment on the github page.
      </div>
      <table align=center width=900px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:28px"><a href='https://github.com/pathak22/modular-assemblies/'>[GitHub]</a></span>
          </td>
        </tr>
      </table>
      <br><hr>

      <table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width=250px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <a href="resources/assemblies.pdf"><img style="height:150px" src="resources/thumbnail.jpeg"/></a>
        <center>
        <span style="font-size:20pt"><a href="resources/assemblies.pdf">[Paper]</a>
        <span style="font-size:20pt"><a href="https://arxiv.org/abs/1902.05546v2">[ArXiv]</a>
        <span style="font-size:20pt"><a href="resources/slides.pdf">[Slides]</a></span>
        <span style="font-size:20pt"><a href="resources/poster.pdf">[Poster]</a></span>
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Deepak Pathak, Chris Lu, Trevor Darrell, Phillip Isola, Alexei A. Efros. <b>Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity<br/></b> In <i>NeurIPS</i> 2019.</span></p>
        <!-- <p style="margin-top:20px;"></p> -->
        <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('assemblies19_bib')" class="togglebib">[Bibtex]</a></span>
        </td>
        </tr>
        <tr>
        <td width=250px align=left>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
          <div class="paper" id="assemblies19_bib">
<pre xml:space="preserve">
@inproceedings{pathak19assemblies,
  Author = {Pathak, Deepak and
  Lu, Chris and Darrell, Trevor and
  Isola, Phillip and Efros, Alexei A.},
  Title = {Learning to Control Self-
  Assembling Morphologies: A Study of
  Generalization via Modularity},
  Booktitle = {NeurIPS},
  Year = {2019}
}</pre>
          </div>
          </td>
          </tr>
      </table>
    <br><hr>

    <table align=center width=800px>
      <tr><td width=800px><left>
      <center><h1>Acknowledgements</h1></center>
      We would like to thank Igor Mordatch, Chris Atkeson, Abhinav Gupta and the members of BAIR for fruitful discussions and comments. This work was supported in part by Berkeley DeepDrive, and the Valrhona reinforcement learning fellowship. DP is supported by the Facebook graduate fellowship.<br>
      </left></td></tr>
    </table>
  <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>
